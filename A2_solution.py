# -*- coding: utf-8 -*-
"""A2-solution.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13MKahGkrfytjWdv3bSoj_iB-Yo4BpFkh

## Importing required libraries
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import tensorflow as tf
from tensorflow import keras
# %matplotlib inline 
import matplotlib.pyplot as plt
np.random.seed(1)
tf.random.set_seed(1)

"""## Defining model"""

def MLP(Input_Dim, Output_Dim, Width, Depth, Act_Func, Reg_Param):  

    assert Depth > 1, 'Depth of generator must be greater than 1'
    
    model = tf.keras.Sequential()
    
    # Adding first hidden layer
    model.add(keras.layers.Dense(Width, input_shape=(Input_Dim,), activation=Act_Func, use_bias=True,
                                     kernel_initializer=keras.initializers.RandomUniform(-1,1), 
                                     bias_initializer=keras.initializers.RandomUniform(-1,1),
                                     kernel_regularizer=keras.regularizers.l2(Reg_Param)))

    # Adding remaining hidden layers
    if(Depth > 2):
        for l in range(Depth - 2):
            #model.add(keras.layers.BatchNormalization())
            model.add(keras.layers.Dense(Width, activation=Act_Func, use_bias=True,
                                     kernel_initializer=keras.initializers.RandomUniform(-1,1), 
                                     bias_initializer=keras.initializers.RandomUniform(-1,1),
                                     kernel_regularizer=keras.regularizers.l2(Reg_Param)))
            
    # Adding output layer
    model.add(keras.layers.Dense(Output_Dim, activation=None, use_bias=True,
                                     kernel_initializer=keras.initializers.RandomUniform(-1,1), 
                                     bias_initializer=keras.initializers.RandomUniform(-1,1),
                                     kernel_regularizer=keras.regularizers.l2(Reg_Param)))

    
    model.compile(optimizer=keras.optimizers.Adam(lr=1.0e-3),
                  loss='mse',
                  metrics=['mse'])
    
    #model.summary()
    
    return model

"""## Training and validation data"""

def target_func(x):
    val = np.sin(10*np.pi*x)
    
    # Adding noise
    val = val + np.random.normal(loc=0.0, scale=0.2, size=val.shape)
    
    return val

Ntrain = 100
Nval   = int(0.25*Ntrain)
N      = Ntrain+Nval

x        = np.linspace(0, 1, N)
y        = target_func(x)

ind1     = np.arange(0,N,5)
ind2     = np.setdiff1d(np.arange(N), ind1)

x_train  = x[ind2]
x_val    = x[ind1]
y_train  = y[ind2]
y_val    = y[ind1]

print(x_train.shape)
print(x_val.shape)


fig, ax  = plt.subplots(figsize=(15, 5))
ax.plot(x_train, y_train,'-o', label='training data')
ax.plot(x_val, y_val,'-x', label='validation data')
plt.legend()
#ax[2].plot(x_test,y_test,'-o')

"""## Evaluating effect of different regularization parameter"""

runs       = 3
reg_param_list = [0.0, 1e-4, 1e-3, 1e-2]

train_err  = [[] for _ in range(len(reg_param_list))]  
val_err    = [[] for _ in range(len(reg_param_list))]
train_pred = [[] for _ in range(len(reg_param_list))]
val_pred   = [[] for _ in range(len(reg_param_list))]

for l in range(len(reg_param_list)):
    print(f'Regularization parameter = {reg_param_list[l]}')
    for run in range(runs):
        print('--- Run '+str(run))
        model = MLP(1, 1, 15, 10, 'tanh', reg_param_list[l])
        history_callback = model.fit(x_train,
                                     y_train,
                                     epochs=5000,
                                     batch_size = 25,
                                     verbose=0,
                                     validation_data=(x_val,y_val),
                                     shuffle=True
                                     )

        train_err[l].append(history_callback.history["mse"])
        val_err[l].append(history_callback.history["val_mse"])
        train_pred[l].append(model.predict(x_train))
        val_pred[l].append(model.predict(x_val))
        print('   ---> final training err = {:.4e}, validation err = {:.4e}'.format(train_err[l][-1][-1],val_err[l][-1][-1]))

fig, ax  = plt.subplots(len(reg_param_list),2,figsize=(15,20))
for l in range(len(reg_param_list)):
    for run in range(runs):
        ax[l,0].semilogy(train_err[l][run],label='run '+str(run))
        ax[l,0].set_xlabel('Epochs')
        ax[l,0].set_ylabel('Training mse')
        ax[l,0].set_ylim([1e-2, 1])
        ax[l,0].legend()
        ax[l,0].set_title('reg param = ' + str(reg_param_list[l]))
        ax[l,1].semilogy(val_err[l][run],label='run '+str(run))
        ax[l,1].legend()
        ax[l,1].set_xlabel('Epochs')
        ax[l,1].set_ylabel('Validation mse')
        ax[l,1].set_ylim([1e-2, 1])
        ax[l,1].set_title('reg param = ' + str(reg_param_list[l]))

fig.tight_layout()

"""## Plotting prediction for training set"""

fig, ax  = plt.subplots(len(reg_param_list),runs,figsize=(15,20))
for l in range(len(reg_param_list)):
    for run in range(runs):
        ax[l,run].plot(x_train,y_train)
        ax[l,run].plot(x_train,train_pred[l][run])
        ax[l,run].set_xlabel('x')
        ax[l,run].set_ylabel('f(x)')
        ax[l,run].set_title('reg param = ' + str(reg_param_list[l]) + ', run = ' + str(run))
fig.tight_layout()

"""## Plotting prediction for validation set"""

fig, ax  = plt.subplots(len(reg_param_list),runs,figsize=(15,20))
for l in range(len(reg_param_list)):
    for run in range(runs):
        ax[l,run].plot(x_val,y_val)
        ax[l,run].plot(x_val,val_pred[l][run])
        ax[l,run].set_xlabel('x')
        ax[l,run].set_ylabel('f(x)')
        ax[l,run].set_title('reg param = ' + str(reg_param_list[l]) + ', run = ' + str(run))
fig.tight_layout()

"""## Q. 5

(a) In the absence of any regularization, how does the network perform (across all 3 re-trains) on the
two datasets?

> In the absesnce of any regularization (reg. parameter=0), the network overfits to the training data, as observed by very low training error, but relatively high validation error. This indicates that the network is trying to "memorize" the training data instead of "learning" the underlying data pattern and generalizing to the out-of-training samples, which is the holy grail of learning from data.

(b) How do the results change when regularization is used?

> When regularization is used (i.e. the cases with non-zero reg. parameter), it helps tackle the adverse effect of overfitting, as observed by relatively similar (closer) training and validation curves. In other words, we are also performing well on out-of-training samples.

(c) Is it useful to use a large regularization? Why or why not?

> It is not useful to use very high regularization parameter, as it will force the network to learn smoother functions which might underfit the training data. This can be observed in both training curve (higher training error) as well as in prediction plots. In fact, the underfitting can be quite severe, as can be seen for run 1 with the parameter 0.01
"""

